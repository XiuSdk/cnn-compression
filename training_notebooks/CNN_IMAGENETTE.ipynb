{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9469 images belonging to 10 classes.\n",
      "Found 3925 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "max_epochs = 80\n",
    "input_shape = (160, 160, 3)\n",
    "weight_decay = 0.0005\n",
    "\n",
    "\n",
    "save_dir = '/home/ec2-user/Telecom/experiments/saved_models'\n",
    "model_name = 'keras_cnn_imagenette_trained_model.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "train_path = '/home/ec2-user/Telecom/experiments/data/imagenette2-320/train'\n",
    "test_path = '/home/ec2-user/Telecom/experiments/data/imagenette2-320/val'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,    \n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(160, 160),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(160, 160),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape= input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(512, kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01)))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "learning_rate = 0.1\n",
    "lr_decay = 1e-6\n",
    "lr_drop = 20\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.001, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=0)\n",
    "\n",
    "early_stop = keras.callbacks.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                     min_delta=0, \n",
    "                                                     patience=8, \n",
    "                                                     verbose=0, \n",
    "                                                     mode='auto', \n",
    "                                                     baseline=None, \n",
    "                                                     restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9469 images belonging to 10 classes.\n",
      "Found 3925 images belonging to 10 classes.\n",
      "Epoch 1/80\n",
      "300/300 [==============================] - 76s 254ms/step - loss: 4.2350 - accuracy: 0.2322 - val_loss: 2.2758 - val_accuracy: 0.3051\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.23553, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 2/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 2.1636 - accuracy: 0.3418 - val_loss: 2.1341 - val_accuracy: 0.3773\n",
      "\n",
      "Epoch 00002: loss improved from 4.23553 to 2.16366, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 3/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 1.9072 - accuracy: 0.4017 - val_loss: 2.2084 - val_accuracy: 0.3723\n",
      "\n",
      "Epoch 00003: loss improved from 2.16366 to 1.90716, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 4/80\n",
      "300/300 [==============================] - 72s 241ms/step - loss: 1.7510 - accuracy: 0.4599 - val_loss: 1.7332 - val_accuracy: 0.4427\n",
      "\n",
      "Epoch 00004: loss improved from 1.90716 to 1.75105, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 5/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.6226 - accuracy: 0.5078 - val_loss: 2.3363 - val_accuracy: 0.5117\n",
      "\n",
      "Epoch 00005: loss improved from 1.75105 to 1.62260, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 6/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.5494 - accuracy: 0.5358 - val_loss: 1.4738 - val_accuracy: 0.5170\n",
      "\n",
      "Epoch 00006: loss improved from 1.62260 to 1.54934, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 7/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.5006 - accuracy: 0.5587 - val_loss: 1.9657 - val_accuracy: 0.4826\n",
      "\n",
      "Epoch 00007: loss improved from 1.54934 to 1.50062, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 8/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.4317 - accuracy: 0.5767 - val_loss: 1.8347 - val_accuracy: 0.5494\n",
      "\n",
      "Epoch 00008: loss improved from 1.50062 to 1.43170, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 9/80\n",
      "300/300 [==============================] - 73s 244ms/step - loss: 1.3940 - accuracy: 0.5962 - val_loss: 2.0238 - val_accuracy: 0.5696\n",
      "\n",
      "Epoch 00009: loss improved from 1.43170 to 1.39396, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 10/80\n",
      "300/300 [==============================] - 73s 244ms/step - loss: 1.3698 - accuracy: 0.6036 - val_loss: 1.1763 - val_accuracy: 0.5902\n",
      "\n",
      "Epoch 00010: loss improved from 1.39396 to 1.36984, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 11/80\n",
      "300/300 [==============================] - 72s 241ms/step - loss: 1.3407 - accuracy: 0.6055 - val_loss: 1.6125 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00011: loss improved from 1.36984 to 1.34076, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 12/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 1.3072 - accuracy: 0.6239 - val_loss: 1.6815 - val_accuracy: 0.5544\n",
      "\n",
      "Epoch 00012: loss improved from 1.34076 to 1.30729, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 13/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.2942 - accuracy: 0.6360 - val_loss: 1.7133 - val_accuracy: 0.5861\n",
      "\n",
      "Epoch 00013: loss improved from 1.30729 to 1.29421, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 14/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.2614 - accuracy: 0.6443 - val_loss: 1.2484 - val_accuracy: 0.6055\n",
      "\n",
      "Epoch 00014: loss improved from 1.29421 to 1.26138, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 15/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 1.2327 - accuracy: 0.6492 - val_loss: 1.6425 - val_accuracy: 0.6074\n",
      "\n",
      "Epoch 00015: loss improved from 1.26138 to 1.23273, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 16/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 1.2224 - accuracy: 0.6518 - val_loss: 1.5600 - val_accuracy: 0.6763\n",
      "\n",
      "Epoch 00016: loss improved from 1.23273 to 1.22250, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 17/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.1949 - accuracy: 0.6626 - val_loss: 1.9207 - val_accuracy: 0.6820\n",
      "\n",
      "Epoch 00017: loss improved from 1.22250 to 1.19495, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 18/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 1.1713 - accuracy: 0.6696 - val_loss: 1.3440 - val_accuracy: 0.6584\n",
      "\n",
      "Epoch 00018: loss improved from 1.19495 to 1.17135, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 19/80\n",
      "300/300 [==============================] - 73s 244ms/step - loss: 1.1594 - accuracy: 0.6765 - val_loss: 1.4628 - val_accuracy: 0.6605\n",
      "\n",
      "Epoch 00019: loss improved from 1.17135 to 1.15938, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 20/80\n",
      "300/300 [==============================] - 73s 244ms/step - loss: 1.1392 - accuracy: 0.6847 - val_loss: 1.2145 - val_accuracy: 0.6824\n",
      "\n",
      "Epoch 00020: loss improved from 1.15938 to 1.13929, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 21/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 1.1257 - accuracy: 0.6880 - val_loss: 1.1230 - val_accuracy: 0.6674\n",
      "\n",
      "Epoch 00021: loss improved from 1.13929 to 1.12571, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 22/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.1057 - accuracy: 0.6987 - val_loss: 1.0809 - val_accuracy: 0.6755\n",
      "\n",
      "Epoch 00022: loss improved from 1.12571 to 1.10569, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 23/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.0968 - accuracy: 0.7037 - val_loss: 1.3043 - val_accuracy: 0.6582\n",
      "\n",
      "Epoch 00023: loss improved from 1.10569 to 1.09674, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 24/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.0641 - accuracy: 0.7092 - val_loss: 1.1277 - val_accuracy: 0.6642\n",
      "\n",
      "Epoch 00024: loss improved from 1.09674 to 1.06421, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 25/80\n",
      "300/300 [==============================] - 73s 245ms/step - loss: 1.0711 - accuracy: 0.7074 - val_loss: 1.2082 - val_accuracy: 0.6590\n",
      "\n",
      "Epoch 00025: loss did not improve from 1.06421\n",
      "Epoch 26/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.0458 - accuracy: 0.7157 - val_loss: 1.3532 - val_accuracy: 0.6692\n",
      "\n",
      "Epoch 00026: loss improved from 1.06421 to 1.04578, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 27/80\n",
      "300/300 [==============================] - 72s 241ms/step - loss: 1.0327 - accuracy: 0.7186 - val_loss: 1.0465 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00027: loss improved from 1.04578 to 1.03270, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 28/80\n",
      "300/300 [==============================] - 73s 244ms/step - loss: 1.0219 - accuracy: 0.7250 - val_loss: 1.9162 - val_accuracy: 0.6371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00028: loss improved from 1.03270 to 1.02199, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 29/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 1.0036 - accuracy: 0.7285 - val_loss: 0.9837 - val_accuracy: 0.7029\n",
      "\n",
      "Epoch 00029: loss improved from 1.02199 to 1.00354, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 30/80\n",
      "300/300 [==============================] - 73s 244ms/step - loss: 1.0059 - accuracy: 0.7294 - val_loss: 0.7297 - val_accuracy: 0.7100\n",
      "\n",
      "Epoch 00030: loss did not improve from 1.00354\n",
      "Epoch 31/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.9895 - accuracy: 0.7359 - val_loss: 0.8697 - val_accuracy: 0.7031\n",
      "\n",
      "Epoch 00031: loss improved from 1.00354 to 0.98943, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 32/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.9743 - accuracy: 0.7413 - val_loss: 1.3718 - val_accuracy: 0.7049\n",
      "\n",
      "Epoch 00032: loss improved from 0.98943 to 0.97430, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 33/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.9547 - accuracy: 0.7445 - val_loss: 0.8999 - val_accuracy: 0.7434\n",
      "\n",
      "Epoch 00033: loss improved from 0.97430 to 0.95476, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 34/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.9543 - accuracy: 0.7451 - val_loss: 1.0267 - val_accuracy: 0.7434\n",
      "\n",
      "Epoch 00034: loss improved from 0.95476 to 0.95428, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 35/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.9597 - accuracy: 0.7438 - val_loss: 0.8324 - val_accuracy: 0.7394\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.95428\n",
      "Epoch 36/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.9368 - accuracy: 0.7510 - val_loss: 1.2449 - val_accuracy: 0.7123\n",
      "\n",
      "Epoch 00036: loss improved from 0.95428 to 0.93666, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 37/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.9279 - accuracy: 0.7511 - val_loss: 1.0694 - val_accuracy: 0.6815\n",
      "\n",
      "Epoch 00037: loss improved from 0.93666 to 0.92804, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 38/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.9347 - accuracy: 0.7540 - val_loss: 1.0432 - val_accuracy: 0.6697\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.92804\n",
      "Epoch 39/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.9071 - accuracy: 0.7637 - val_loss: 0.9885 - val_accuracy: 0.7394\n",
      "\n",
      "Epoch 00039: loss improved from 0.92804 to 0.90704, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 40/80\n",
      "300/300 [==============================] - 72s 241ms/step - loss: 0.8870 - accuracy: 0.7649 - val_loss: 0.8360 - val_accuracy: 0.7400\n",
      "\n",
      "Epoch 00040: loss improved from 0.90704 to 0.88710, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 41/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.9173 - accuracy: 0.7551 - val_loss: 0.7637 - val_accuracy: 0.7229\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.88710\n",
      "Epoch 42/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.8885 - accuracy: 0.7631 - val_loss: 0.9063 - val_accuracy: 0.7392\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.88710\n",
      "Epoch 43/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.8699 - accuracy: 0.7665 - val_loss: 0.8791 - val_accuracy: 0.7248\n",
      "\n",
      "Epoch 00043: loss improved from 0.88710 to 0.86984, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 44/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.8705 - accuracy: 0.7725 - val_loss: 0.8656 - val_accuracy: 0.7292\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.86984\n",
      "Epoch 45/80\n",
      "300/300 [==============================] - 73s 244ms/step - loss: 0.8774 - accuracy: 0.7644 - val_loss: 1.2606 - val_accuracy: 0.7315\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.86984\n",
      "Epoch 46/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.8822 - accuracy: 0.7641 - val_loss: 0.8101 - val_accuracy: 0.7426\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.86984\n",
      "Epoch 47/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.8385 - accuracy: 0.7832 - val_loss: 0.9244 - val_accuracy: 0.7490\n",
      "\n",
      "Epoch 00047: loss improved from 0.86984 to 0.83838, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 48/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.8695 - accuracy: 0.7701 - val_loss: 0.8860 - val_accuracy: 0.7125\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.83838\n",
      "Epoch 49/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.8383 - accuracy: 0.7816 - val_loss: 1.1800 - val_accuracy: 0.7118\n",
      "\n",
      "Epoch 00049: loss improved from 0.83838 to 0.83833, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 50/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.8413 - accuracy: 0.7790 - val_loss: 0.8509 - val_accuracy: 0.7300\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.83833\n",
      "Epoch 51/80\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.8332 - accuracy: 0.7815 - val_loss: 0.9466 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00051: loss improved from 0.83833 to 0.83317, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 52/80\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.8363 - accuracy: 0.7774 - val_loss: 1.2518 - val_accuracy: 0.7256\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.83317\n",
      "Epoch 53/80\n",
      " 79/300 [======>.......................] - ETA: 41s - loss: 0.7891 - accuracy: 0.7933"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-63a7d49fda59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m           \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m           \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m )\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow2_p27/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow2_p27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow2_p27/lib/python2.7/site-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow2_p27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow2_p27/lib/python2.7/site-packages/tensorflow_core/python/keras/backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3746\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3748\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3749\u001b[0m         expand_composites=True)\n\u001b[1;32m   3750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "          train_generator,\n",
    "          epochs=max_epochs,\n",
    "          validation_data=validation_generator,\n",
    "          steps_per_epoch = 300,\n",
    "          validation_steps = 150,\n",
    "          verbose = 1,\n",
    "          callbacks=[model_checkpoint]\n",
    ")\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(model.history.history['accuracy'])\n",
    "plt.plot(model.history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "300/300 [==============================] - 76s 254ms/step - loss: 0.8216 - accuracy: 0.7889 - val_loss: 0.9054 - val_accuracy: 0.7611\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.82165, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.8104 - accuracy: 0.7867 - val_loss: 0.9560 - val_accuracy: 0.7519\n",
      "\n",
      "Epoch 00002: loss improved from 0.82165 to 0.81039, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.8089 - accuracy: 0.7909 - val_loss: 1.1387 - val_accuracy: 0.7189\n",
      "\n",
      "Epoch 00003: loss improved from 0.81039 to 0.80902, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 72s 241ms/step - loss: 0.8144 - accuracy: 0.7855 - val_loss: 0.9835 - val_accuracy: 0.7584\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.80902\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.7894 - accuracy: 0.7965 - val_loss: 0.9217 - val_accuracy: 0.7652\n",
      "\n",
      "Epoch 00005: loss improved from 0.80902 to 0.78946, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 72s 241ms/step - loss: 0.7891 - accuracy: 0.7982 - val_loss: 0.7009 - val_accuracy: 0.7707\n",
      "\n",
      "Epoch 00006: loss improved from 0.78946 to 0.78917, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 73s 242ms/step - loss: 0.7888 - accuracy: 0.7908 - val_loss: 1.1047 - val_accuracy: 0.7617\n",
      "\n",
      "Epoch 00007: loss improved from 0.78917 to 0.78872, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 72s 241ms/step - loss: 0.7831 - accuracy: 0.7954 - val_loss: 0.6626 - val_accuracy: 0.7409\n",
      "\n",
      "Epoch 00008: loss improved from 0.78872 to 0.78314, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 72s 242ms/step - loss: 0.7798 - accuracy: 0.7989 - val_loss: 1.4747 - val_accuracy: 0.7818\n",
      "\n",
      "Epoch 00009: loss improved from 0.78314 to 0.77988, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.7704 - accuracy: 0.7988 - val_loss: 1.0700 - val_accuracy: 0.7591\n",
      "\n",
      "Epoch 00010: loss improved from 0.77988 to 0.77036, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.7688 - accuracy: 0.7993 - val_loss: 1.1781 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00011: loss improved from 0.77036 to 0.76884, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 73s 244ms/step - loss: 0.7650 - accuracy: 0.8020 - val_loss: 1.0547 - val_accuracy: 0.7810\n",
      "\n",
      "Epoch 00012: loss improved from 0.76884 to 0.76501, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.7705 - accuracy: 0.8004 - val_loss: 0.8327 - val_accuracy: 0.7546\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.76501\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 72s 241ms/step - loss: 0.7562 - accuracy: 0.8029 - val_loss: 1.4065 - val_accuracy: 0.7664\n",
      "\n",
      "Epoch 00014: loss improved from 0.76501 to 0.75617, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 73s 243ms/step - loss: 0.7386 - accuracy: 0.8096 - val_loss: 0.8992 - val_accuracy: 0.7292\n",
      "\n",
      "Epoch 00015: loss improved from 0.75617 to 0.73865, saving model to /home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa2ec33ab10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart training\n",
    "from keras.models import load_model\n",
    "model_checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "model = load_model(model_path)\n",
    "model.fit_generator(\n",
    "          train_generator,\n",
    "          epochs=15,\n",
    "          validation_data=validation_generator,\n",
    "          steps_per_epoch = 300,\n",
    "          validation_steps = 150,\n",
    "          verbose = 1,\n",
    "          callbacks=[model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(model.history.history['accuracy'])\n",
    "plt.plot(model.history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_61 (Conv2D)           (None, 160, 160, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 160, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 158, 158, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 158, 158, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 79, 79, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 79, 79, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 79, 79, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 79, 79, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 77, 77, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 77, 77, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 38, 38, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 38, 38, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 36, 36, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               8389120   \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 9,566,506\n",
      "Trainable params: 9,566,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/home/ec2-user/Telecom/experiments/saved_models/keras_cnn_imagenette_trained_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p27)",
   "language": "python",
   "name": "conda_tensorflow2_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
